{
  "seed": 0,
  "device": "cuda:2",
  "num_epochs": 5,
  "max_seq_length":512, 
  "per_device_train_batch_size" : 32,
  "logging_per_n_step" : 1000,
  "path_tokenizer": "./RoBERTa/RoBERTa_Tokenizer_extend",
  "path_data_train": "./babylm_data/babylm_100M/all_100_zzyyh.train",
  "path_data_dev": "./babylm_data/babylm_dev/all.dev",

  "model_vocab_size": 52000,
  "model_max_position_embeddings": 512,
  "model_num_attention_heads": 12,
  "model_num_hidden_layers": 12,
  "model_type_vocab_size": 1,

  "path_folder_prefix": "",
  "mlm_probability": 0.15
}